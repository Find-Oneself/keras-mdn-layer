{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A Mixture Density Layer for Keras\n",
    "cpmpercussion: Charles Martin (University of Oslo) 2018\n",
    "https://github.com/cpmpercussion/keras-mdn-layer\n",
    "\n",
    "Hat tip to [Omimo's Keras MDN layer](https://github.com/omimo/Keras-MDN) for a starting point for this code.\n",
    "\"\"\"\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense\n",
    "from keras.engine.topology import Layer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "def elu_plus_one_plus_epsilon(x):\n",
    "    \"\"\"ELU activation with a very small addition to help prevent NaN in loss.\"\"\"\n",
    "    return (K.elu(x) + 1 + 1e-8)\n",
    "\n",
    "\n",
    "class MDN(Layer):\n",
    "    \"\"\"A Mixture Density Network Layer for Keras.\n",
    "    This layer has a few tricks to avoid NaNs in the loss function when training:\n",
    "        - Activation for variances is ELU + 1 + 1e-8 (to avoid very small values)\n",
    "        - Mixture weights (pi) are trained in as logits, not in the softmax space.\n",
    "\n",
    "    A loss function needs to be constructed with the same output dimension and number of mixtures.\n",
    "    A sampling function is also provided to sample from distribution parametrised by the MDN outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dimension, num_mixtures, **kwargs):\n",
    "        self.output_dim = output_dimension\n",
    "        self.num_mix = num_mixtures\n",
    "        with tf.name_scope('MDN'):\n",
    "            self.mdn_mus = Dense(self.num_mix * self.output_dim, name='mdn_mus')  # mix*output vals, no activation\n",
    "            self.mdn_sigmas = Dense(self.num_mix * self.output_dim, activation=elu_plus_one_plus_epsilon, name='mdn_sigmas')  # mix*output vals exp activation\n",
    "            self.mdn_pi = Dense(self.num_mix, name='mdn_pi')  # mix vals, logits\n",
    "        super(MDN, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.mdn_mus.build(input_shape)\n",
    "        self.mdn_sigmas.build(input_shape)\n",
    "        self.mdn_pi.build(input_shape)\n",
    "        self.trainable_weights = self.mdn_mus.trainable_weights + self.mdn_sigmas.trainable_weights + self.mdn_pi.trainable_weights\n",
    "        self.non_trainable_weights = self.mdn_mus.non_trainable_weights + self.mdn_sigmas.non_trainable_weights + self.mdn_pi.non_trainable_weights\n",
    "        super(MDN, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        with tf.name_scope('MDN'):\n",
    "            mdn_out = keras.layers.concatenate([self.mdn_mus(x),\n",
    "                                                self.mdn_sigmas(x),\n",
    "                                                self.mdn_pi(x)],\n",
    "                                               name='mdn_outputs')\n",
    "        return mdn_out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], (2 * self.output_dim * self.num_mix) + self.num_mix)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"output_dimension\": self.output_dim,\n",
    "            \"num_mixtures\": self.num_mix\n",
    "        }\n",
    "        base_config = super(MDN, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "def get_mixture_loss_func(output_dim, num_mixes):\n",
    "    \"\"\"Construct a loss functions for the MDN layer parametrised by number of mixtures.\"\"\"\n",
    "    # Construct a loss function with the right number of mixtures and outputs\n",
    "    def loss_func(y_true, y_pred):\n",
    "        out_mu, out_sigma, out_pi = tf.split(y_pred, num_or_size_splits=[num_mixes * output_dim,\n",
    "                                                                         num_mixes * output_dim,\n",
    "                                                                         num_mixes],\n",
    "                                             axis=1, name='mdn_coef_split')\n",
    "        cat = tfd.Categorical(logits=out_pi)\n",
    "        component_splits = [output_dim] * num_mixes\n",
    "        mus = tf.split(out_mu, num_or_size_splits=component_splits, axis=1)\n",
    "        sigs = tf.split(out_sigma, num_or_size_splits=component_splits, axis=1)\n",
    "        coll = [tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale) for loc, scale\n",
    "                in zip(mus, sigs)]\n",
    "        mixture = tfd.Mixture(cat=cat, components=coll)\n",
    "        loss = mixture.log_prob(y_true)\n",
    "        loss = tf.negative(loss)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss\n",
    "\n",
    "    # Actually return the loss_func\n",
    "    with tf.name_scope('MDN'):\n",
    "        return loss_func\n",
    "\n",
    "\n",
    "def get_mixture_sampling_fun(output_dim, num_mixes):\n",
    "    \"\"\"Construct a sampling function for the MDN layer parametrised by mixtures and output dimension.\"\"\"\n",
    "    # Construct a loss function with the right number of mixtures and outputs\n",
    "    def sampling_func(y_pred):\n",
    "        out_mu, out_sigma, out_pi = tf.split(y_pred, num_or_size_splits=[num_mixes * output_dim,\n",
    "                                                                         num_mixes * output_dim,\n",
    "                                                                         num_mixes],\n",
    "                                             axis=1, name='mdn_coef_split')\n",
    "        cat = tfd.Categorical(logits=out_pi)\n",
    "        component_splits = [output_dim] * num_mixes\n",
    "        mus = tf.split(out_mu, num_or_size_splits=component_splits, axis=1)\n",
    "        sigs = tf.split(out_sigma, num_or_size_splits=component_splits, axis=1)\n",
    "        coll = [tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale) for loc, scale\n",
    "                in zip(mus, sigs)]\n",
    "        mixture = tfd.Mixture(cat=cat, components=coll)\n",
    "        samp = mixture.sample()\n",
    "        # Todo: temperature adjustment for sampling function.\n",
    "        return samp\n",
    "\n",
    "    # Actually return the loss_func\n",
    "    with tf.name_scope('MDNLayer'):\n",
    "        return sampling_func\n",
    "\n",
    "\n",
    "def get_mixture_mse_accuracy(output_dim, num_mixes):\n",
    "    \"\"\"Construct an MSE accuracy function for the MDN layer\n",
    "    that takes one sample and compares to the true value.\"\"\"\n",
    "    # Construct a loss function with the right number of mixtures and outputs\n",
    "    def mse_func(y_true, y_pred):\n",
    "        out_mu, out_sigma, out_pi = tf.split(y_pred, num_or_size_splits=[num_mixes * output_dim,\n",
    "                                                                         num_mixes * output_dim,\n",
    "                                                                         num_mixes],\n",
    "                                             axis=1, name='mdn_coef_split')\n",
    "        cat = tfd.Categorical(logits=out_pi)\n",
    "        component_splits = [output_dim] * num_mixes\n",
    "        mus = tf.split(out_mu, num_or_size_splits=component_splits, axis=1)\n",
    "        sigs = tf.split(out_sigma, num_or_size_splits=component_splits, axis=1)\n",
    "        coll = [tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale) for loc, scale\n",
    "                in zip(mus, sigs)]\n",
    "        mixture = tfd.Mixture(cat=cat, components=coll)\n",
    "        samp = mixture.sample()\n",
    "        mse = tf.reduce_mean(tf.square(samp - y_true), axis=-1)\n",
    "        # Todo: temperature adjustment for sampling functon.\n",
    "        return mse\n",
    "\n",
    "    # Actually return the loss_func\n",
    "    with tf.name_scope('MDNLayer'):\n",
    "        return mse_func\n",
    "\n",
    "\n",
    "def split_mixture_params(params, output_dim, num_mixes):\n",
    "    \"\"\"Splits up an array of mixture parameters into mus, sigmas, and pis\n",
    "    depending on the number of mixtures and output dimension.\"\"\"\n",
    "    mus = params[:num_mixes*output_dim]\n",
    "    sigs = params[num_mixes*output_dim:2*num_mixes*output_dim]\n",
    "    pi_logits = params[-num_mixes:]\n",
    "    return mus, sigs, pi_logits\n",
    "\n",
    "\n",
    "def softmax(w, t=1.0):\n",
    "    \"\"\"Softmax function for a list or numpy array of logits. Also adjusts temperature.\"\"\"\n",
    "    e = np.array(w) / t  # adjust temperature\n",
    "    e -= e.max()  # subtract max to protect from exploding exp values.\n",
    "    e = np.exp(e)\n",
    "    dist = e / np.sum(e)\n",
    "    return dist\n",
    "\n",
    "\n",
    "def sample_from_categorical(dist):\n",
    "    \"\"\"Samples from a categorical model PDF.\"\"\"\n",
    "    r = np.random.rand(1)  # uniform random number in [0,1]\n",
    "    accumulate = 0\n",
    "    for i in range(0, dist.size):\n",
    "        accumulate += dist[i]\n",
    "        if accumulate >= r:\n",
    "            return i\n",
    "    tf.logging.info('Error sampling mixture model.')\n",
    "    return -1\n",
    "\n",
    "\n",
    "def sample_from_output(params, output_dim, num_mixes, temp=1.0):\n",
    "    \"\"\"Sample from an MDN output with temperature adjustment.\"\"\"\n",
    "    mus = params[:num_mixes*output_dim]\n",
    "    sigs = params[num_mixes*output_dim:2*num_mixes*output_dim]\n",
    "    pis = softmax(params[-num_mixes:], t=temp)\n",
    "    m = sample_from_categorical(pis)\n",
    "    # Alternative way to sample from categorical:\n",
    "    # m = np.random.choice(range(len(pis)), p=pis)\n",
    "    mus_vector = mus[m*output_dim:(m+1)*output_dim]\n",
    "    sig_vector = sigs[m*output_dim:(m+1)*output_dim] * temp  # adjust for temperature\n",
    "    cov_matrix = np.identity(output_dim) * sig_vector\n",
    "    sample = np.random.multivariate_normal(mus_vector, cov_matrix, 1)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Training Hyperparameters:\n",
    "SEQ_LEN = 30\n",
    "BATCH_SIZE = 64\n",
    "HIDDEN_UNITS = 256\n",
    "EPOCHS = 100\n",
    "SEED = 2345  # set random seed for reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "OUTPUT_DIMENSION = 3\n",
    "NUMBER_MIXTURES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_34 (LSTM)               (None, 30, 256)           266240    \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, 30, 70)            17990     \n",
      "=================================================================\n",
      "Total params: 284,230\n",
      "Trainable params: 284,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_34 (LSTM)               (None, 30, 256)           266240    \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, 30, 70)            17990     \n",
      "=================================================================\n",
      "Total params: 284,230\n",
      "Trainable params: 284,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_mixes = NUMBER_MIXTURES\n",
    "output_dim = OUTPUT_DIMENSION\n",
    "\n",
    "def loss_func(y_true, y_pred):\n",
    "    # Reshape inputs in case this is used in a TimeDistribued layer\n",
    "    y_pred = tf.reshape(y_pred, [-1, (2 * num_mixes * output_dim) + num_mixes], name='reshape_ypreds')\n",
    "    y_true = tf.reshape(y_true, [-1, output_dim], name='reshape_ytrue')\n",
    "    # Split the inputs into paramaters\n",
    "    out_mu, out_sigma, out_pi = tf.split(y_pred, num_or_size_splits=[num_mixes * output_dim,\n",
    "                                                                     num_mixes * output_dim,\n",
    "                                                                     num_mixes],\n",
    "                                         axis=-1, name='mdn_coef_split')\n",
    "    # Construct the mixture models\n",
    "    cat = tfd.Categorical(logits=out_pi)\n",
    "    component_splits = [output_dim] * num_mixes\n",
    "    mus = tf.split(out_mu, num_or_size_splits=component_splits, axis=1)\n",
    "    sigs = tf.split(out_sigma, num_or_size_splits=component_splits, axis=1)\n",
    "    coll = [tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale) for loc, scale\n",
    "            in zip(mus, sigs)]\n",
    "    mixture = tfd.Mixture(cat=cat, components=coll)\n",
    "    loss = mixture.log_prob(y_true)\n",
    "    loss = tf.negative(loss)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "# Sequential model\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.LSTM(HIDDEN_UNITS, batch_input_shape=(None,SEQ_LEN,OUTPUT_DIMENSION), return_sequences=True))\n",
    "model.add(keras.layers.TimeDistributed(MDN(OUTPUT_DIMENSION, NUMBER_MIXTURES)))\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss=loss_func, optimizer='adam')\n",
    "\n",
    "# Let's see what we have:\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixes: 10\n",
      "Output Dim: 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 30, 3)             0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               266240    \n",
      "_________________________________________________________________\n",
      "mdn_outputs (MDN)            (None, 70)                17990     \n",
      "=================================================================\n",
      "Total params: 284,230\n",
      "Trainable params: 284,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(?, 30, 3)\n",
      "(?, 256)\n",
      "(?, 70)\n"
     ]
    }
   ],
   "source": [
    "num_mixes = NUMBER_MIXTURES\n",
    "output_dim = OUTPUT_DIMENSION\n",
    "\n",
    "print(\"Mixes:\", num_mixes)\n",
    "print(\"Output Dim:\", output_dim)\n",
    "\n",
    "def loss_func(y_true, y_pred):\n",
    "    out_mu, out_sigma, out_pi = tf.split(y_pred, num_or_size_splits=[num_mixes * output_dim,\n",
    "                                                                     num_mixes * output_dim,\n",
    "                                                                     num_mixes],\n",
    "                                         axis=-1, name='mdn_coef_split')\n",
    "    cat = tfd.Categorical(logits=out_pi)\n",
    "    component_splits = [output_dim] * num_mixes\n",
    "    mus = tf.split(out_mu, num_or_size_splits=component_splits, axis=1)\n",
    "    sigs = tf.split(out_sigma, num_or_size_splits=component_splits, axis=1)\n",
    "    coll = [tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale) for loc, scale\n",
    "            in zip(mus, sigs)]\n",
    "    mixture = tfd.Mixture(cat=cat, components=coll)\n",
    "    loss = mixture.log_prob(y_true)\n",
    "    loss = tf.negative(loss)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "inputs = keras.layers.Input(shape=(SEQ_LEN,OUTPUT_DIMENSION), name='inputs')\n",
    "lstm_out = keras.layers.LSTM(HIDDEN_UNITS, name='lstm')(inputs)\n",
    "mdn_out = MDN(OUTPUT_DIMENSION, NUMBER_MIXTURES, name='mdn_outputs')(lstm_out)\n",
    "model = keras.models.Model(inputs=inputs, outputs=mdn_out)\n",
    "model.compile(loss=loss_func, optimizer='adam')\n",
    "model.summary()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
